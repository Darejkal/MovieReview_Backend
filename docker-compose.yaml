services:
  test-producer:
    build: 
      context: ./backend
      target: tester-producer
    entrypoint: ['python','-u','/test/main.py']
    volumes:
      - ./backend/test:/test
    env_file: env/dev.env
    restart: on-failure
    depends_on:
      - producer
  producer:
    build: &producer-build 
      context: ./backend
      target: producer
    volumes:
      - ./backend/producer/main.py:/main.py
    entrypoint: ['python','-u','-m','gunicorn', '--bind', '0.0.0.0:8000', 'main:app']
    depends_on:
      - redpanda
      - kafka-admin
    restart: always
    env_file: env/dev.env
  consumer:
    build: 
      context: ./backend
      target: producer
    volumes:
      - ./backend/consumer/main.py:/main.py
    entrypoint: ['python','-u','/main.py']
    depends_on:
      - redpanda
      - postgres
    restart: always
    env_file: env/dev.env
  # model-trainer:
  #   build: 
  #     context: ./backend
  #     target: model-trainer
  #   volumes:
  #     - ./backend/model_trainer/main.py:/main.py
  #     - ./model:/model
  #   entrypoint: ['python','-u','/main.py']
  #   depends_on:
  #     - redpanda
  #   restart: always
  #   env_file: env/dev.env
  kafka-admin:
    build: *producer-build
    volumes:
      - ./backend/admin/main.py:/main.py
    entrypoint: ['python','/main.py']
    env_file: env/dev.env
    depends_on:
      - redpanda
    restart: on-failure
  redpanda:
    image: docker.redpanda.com/vectorized/redpanda:v22.2.2
    command:
      - redpanda start
      - --smp 1
      - --overprovisioned
      - --node-id 0
      - --kafka-addr PLAINTEXT://0.0.0.0:9092,OUTSIDE://0.0.0.0:19092
      - --advertise-kafka-addr PLAINTEXT://redpanda:9092,OUTSIDE://0.0.0.0:19092
      - --pandaproxy-addr internal://0.0.0.0:8082,external://0.0.0.0:18082
      - --advertise-pandaproxy-addr internal://redpanda:8082,external://0.0.0.0:18082
      - --schema-registry-addr internal://0.0.0.0:8081,external://0.0.0.0:18081
      - --rpc-addr redpanda:33145
      - --advertise-rpc-addr redpanda:33145
      - --mode dev-container
      - --default-log-level=debug
    ports:
      - 18081:18081
      - 18082:18082
      - 19092:19092
      - 19644:9644
  console:
    container_name: redpanda-console
    image: docker.redpanda.com/redpandadata/console:v2.3.8
    entrypoint: /bin/sh
    command: -c 'echo "$$CONSOLE_CONFIG_FILE" > /tmp/config.yml; /app/console'
    environment:
      CONFIG_FILEPATH: /tmp/config.yml
      CONSOLE_CONFIG_FILE: |
        kafka:
          brokers: ["redpanda:9092"]
          schemaRegistry:
            enabled: true
            urls: ["http://redpanda:8081"]
        redpanda:
          adminApi:
            enabled: true
            urls: ["http://redpanda:9644"]
    ports:
      - 18083:8080
    depends_on:
      - redpanda
  postgres:
    image: postgres:16.1-alpine
    env_file:
    - ./env/dev.env
    # volumes:
    # - pgdata:/var/lib/postgresql/data
    restart: on-failure
  adminer:
    image: adminer
    restart: always
    ports:
      - 38080:8080
  pyspark-client:
    build: 
      context: ./backend
      target: spark-sbert2
    ports:
      - "4040:4040"
    volumes:
      - ./backend/pyspark-client/main.py:/main.py
      - ./model:/model
    entrypoint: bash -c "$${SPARK_HOME}/bin/spark-submit \
      --deploy-mode client \
      --driver-class-path local:///opt/spark/jars/postgresql.jar\
      --jars local:///opt/spark/jars/spark-sql-kafka.jar,local:///opt/spark/jars/kafka-clients.jar,local:///opt/spark/jars/commons-pool.jar,local:///opt/spark/jars/spark-streaming-kafka.jar,local:///opt/spark/jars/spark-token-provider.jar,local:///opt/spark/jars/postgresql.jar\
      /main.py"
    depends_on:
      - redpanda
    env_file: env/dev.env
    # restart: always
  spark-master:
    build:
      context: ./backend
      target: spark-sbert2
    ports:
      - "28080:8080"
      - "7077:7077"
    environment:
      - SPARK_LOCAL_IP=spark-master
      - SPARK_WORKLOAD=master
    command: bash -c "$${SPARK_HOME}/bin/spark-class org.apache.spark.deploy.master.Master"
    restart: always
    env_file: env/dev.env
  spark-worker-1:
    build:
      context: ./backend
      target: spark-sbert2
    ports:
      - "28081:8081"
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_DRIVER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker-1
    command: bash -c "$${SPARK_HOME}/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077"
  spark-worker-2:
    build:
      context: ./backend
      target: spark-sbert2
    ports:
      - "28082:8081"
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_DRIVER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker-2

    command: bash -c "$${SPARK_HOME}/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077"
  frontend:
    build: 
      context: ./frontend
    volumes:
      - ./frontend/flask:/flask
    entrypoint: ['python','-u','/flask/main.py']
    depends_on:
      - redpanda
      - kafka-admin
    ports:
    - "5000:5000"
    restart: always
    env_file: env/dev.env
  namenode:
    image: apache/hadoop:3
    hostname: namenode
    command: ["hdfs", "namenode"]
    ports:
      - 9870:9870
    env_file:
      - ./env/hadoop.env
    environment:
        ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
  datanode:
    image: apache/hadoop:3
    command: ["hdfs", "datanode"]
    env_file:
      - ./env/hadoop.env      
  resourcemanager:
    image: apache/hadoop:3
    hostname: resourcemanager
    command: ["yarn", "resourcemanager"]
    ports:
        - 8088:8088
    env_file:
      - ./env/hadoop.env
    volumes:
      - ./test.sh:/opt/test.sh
  nodemanager:
    image: apache/hadoop:3
    command: ["yarn", "nodemanager"]
    env_file:
      - ./env/hadoop.env
  # demo-database:
  #   image: postgres:11.7-alpine
  #   ports: 
  #     - "5432:5432"
  #   environment: 
  #     - POSTGRES_PASSWORD=casa1234
  # consumer:
  # model:
  # hook:
  # deploy:
  # fluentd:
  # elastic:
  # postgres:
